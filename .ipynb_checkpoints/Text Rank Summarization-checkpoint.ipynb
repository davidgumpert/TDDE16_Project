{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import Packages"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "All packages that are needed are imported in the next cell, should only be ran once. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: rouge in c:\\users\\david\\anaconda3\\envs\\tdde16\\lib\\site-packages (1.0.1)\n",
      "Requirement already satisfied: six in c:\\users\\david\\anaconda3\\envs\\tdde16\\lib\\site-packages (from rouge) (1.15.0)\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "!{sys.executable} -m pip install rouge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gzip as gz\n",
    "import json\n",
    "import sys as sklearn\n",
    "import spacy as sp\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "import networkx as nx\n",
    "import math\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from nltk.cluster.util import cosine_distance\n",
    "from rouge import Rouge"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = \"newsroom-release/release/dev.jsonl.gz\"\n",
    "data = []\n",
    "\n",
    "with gz.open(path) as f:\n",
    "    for ln in f:\n",
    "        obj = json.loads(ln)\n",
    "        data.append(obj)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Extracting appropriate data and transforming to pandas dataframe\n",
    "df = pd.DataFrame(data)\n",
    "df_extractive = df[df.density_bin == 'extractive']\n",
    "\n",
    "# Filtering on rough estimate of lenght text\n",
    "article_lengths = [len(text.split()) for text in df_extractive.text]\n",
    "_ = plt.hist(article_lengths, bins = 100, range = (0, 2000))\n",
    "length_check = [len > 250 for len in article_lengths]\n",
    "df_extractive = df_extractive[length_check]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Intermidiate Input Representation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sentence splitting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = sp.load(\"en_core_web_sm\", disable=[\"tagger\", \"parser\", \"ner\", \"textcat\"])\n",
    "nlp.add_pipe(nlp.create_pipe('sentencizer'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SENTENCE SPLITTING FUNCTION\n",
    "def sentence_splitting(doc):\n",
    "    # Using the NLP sentencizer pipeline to extract all text sentences\n",
    "    doc = nlp(doc['text'])\n",
    "    sentences = [sent.string.strip() for sent in doc.sents]\n",
    "    \n",
    "    return sentences"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sparse TF-ISF matrix representation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PREPROCESSING FUNCTION\n",
    "def preprocess(text):\n",
    "    # Transform text with SpaCy model for NLP procedures\n",
    "    text = nlp(text)\n",
    "    \n",
    "    # loop through the words in the text, removing stopwords and numerics\n",
    "    # Assign the remaining tokens to the token list in the lemma form\n",
    "    tokens = []\n",
    "    for token in text:\n",
    "        if token.is_stop == False and token.is_alpha == True:\n",
    "            tokens.append(token.lemma_)\n",
    "    \n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TF-ISF MATRIX CREATION FUNCTION\n",
    "def create_tfisf_matrix(sentences, preprocessor = preprocess):\n",
    "    #Initializing ScikitLearn TF-IDF vectorizer and creating TF-IDF sparse matrix\n",
    "    vectorizer = TfidfVectorizer(tokenizer = preprocessor)\n",
    "    tfisf_matrix = vectorizer.fit_transform(sentences)\n",
    "    \n",
    "    # Saving list of all corpus tokens\n",
    "    feature_names = vectorizer.get_feature_names()\n",
    "\n",
    "    # Returning TF-IDF matrix\n",
    "    return tfisf_matrix, feature_names"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Similarity matrix "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SIMILARITY MATRIX FUNCTIUON\n",
    "def create_sim_matrix(sentences, tfisf_matrix):\n",
    "    # Remove possible NaN or inf or -inf and replace with numerical value\n",
    "    tfisf_matrix = np.nan_to_num(tfisf_matrix)\n",
    "    \n",
    "    # Transform matrix to list representation, needed for the cosine distance function\n",
    "    tfisf_matrix_list = tfisf_matrix.toarray().tolist()\n",
    "    \n",
    "    # Initialize empty quadratic sentence similarity matrix\n",
    "    similarity_matrix = np.zeros((len(sentences), len(sentences)))\n",
    "    \n",
    "    # Loop over all similarity matrix positions, calculating the cosine similarity sentence i,j\n",
    "    for i in range(len(sentences)):\n",
    "        # Ignore if sentence has no token, avoids cosine distance calculation of zero-vector\n",
    "        if sum(tfisf_matrix_list[i]) == 0:\n",
    "            continue\n",
    "        for j in range(len(sentences)):\n",
    "            # Ignore if both are same sentences or sentence has no token\n",
    "            if i == j or sum(tfisf_matrix_list[j]) == 0: \n",
    "                continue \n",
    "            similarity_matrix[i][j] = 1 - cosine_distance(tfisf_matrix_list[i], tfisf_matrix_list[j])\n",
    "    \n",
    "    # Return the final similarity matrix\n",
    "    return similarity_matrix\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Graph representation and sentence scoring"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SENTENCE SCORING FUNCTION\n",
    "def textrank_sentence_scoring(similarity_matrix, sentences):\n",
    "    # Create graphical representation from similarity matrix\n",
    "    graph = nx.from_numpy_array(similarity_matrix)\n",
    "    \n",
    "    # Rank all sentences according to pagerank algorithm\n",
    "    sentence_scores = nx.pagerank(graph, max_iter = 500)\n",
    "    \n",
    "    # Sort all sentences and sentence index according to score\n",
    "    sentence_scores_sorted = sorted(((sentence_scores[i],i) for i,s in enumerate(sentences)), reverse=True)    \n",
    "    \n",
    "    # Return the sorted sentence scores + index \n",
    "    return sentence_scores_sorted"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sentence Extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def textrank_sentence_extraction(sentences, sentence_scores, n):\n",
    "    summary = []\n",
    "    \n",
    "    # Sort sentence indices in ascending order\n",
    "    sentence_indices_sorted = sorted(sentence_scores[0:n], key=lambda tup: tup[1])\n",
    "    \n",
    "    # Extract sentences and append to summary in that order\n",
    "    for i in sentence_indices_sorted:\n",
    "        summary.append(sentences[i[1]])\n",
    "    \n",
    "    # Join all sentences to form final summary\n",
    "    summary = ' '.join(summary)\n",
    "    \n",
    "    # Return summary\n",
    "    return summary\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " # Running the Model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setting corpus size to evaluate\n",
    "corpus = df_extractive.sample(5000)\n",
    "corpus.reset_index(inplace = True)\n",
    "summaries = []\n",
    "\n",
    "# Looping over every document in every corpus, running each model step\n",
    "for index, doc in corpus.iterrows():\n",
    "    # Sentence split document\n",
    "    sentences = sentence_splitting(doc)\n",
    "    \n",
    "    # TF-ISF matrix construction\n",
    "    tfisf_matrix, feature_names = create_tfisf_matrix(sentences = sentences)\n",
    "    \n",
    "    # Similarity matrix construction\n",
    "    similarity_matrix = create_sim_matrix(sentences, tfisf_matrix)\n",
    "    \n",
    "    # Scoring each document sentence\n",
    "    sentence_scores = textrank_sentence_scoring(similarity_matrix, sentences)\n",
    "    \n",
    "    # Extracting and merging sentences \n",
    "    summary = textrank_sentence_extraction(sentences, sentence_scores, 3)\n",
    "    \n",
    "    # Append summary to list of summaries \n",
    "    summaries.append(summary)\n",
    "    \n",
    "    # Checkpoint alert\n",
    "    if index != 0 and index % 5000 == 0:\n",
    "        print('Checkpoint: ', index)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Score summaries with ROUGE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ROUGE AND BLEU SCORING FUNCTION\n",
    "def rouge_blue_scoring(summary, reference):\n",
    "    rouge_score = rouge.get_scores(summary, reference)\n",
    "    \n",
    "    return rouge_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "rouge = Rouge()\n",
    "scores = rouge_blue_scoring(summaries, corpus.iloc[0:len(summaries)].summary.values.tolist())\n",
    "scores_pd = pd.DataFrame(scores)\n",
    "\n",
    "rouge1_f1_scores = []\n",
    "rouge1_recall_scores = []\n",
    "rouge1_precision_scores = []\n",
    "\n",
    "rouge2_f1_scores = []\n",
    "rouge2_recall_scores = []\n",
    "rouge2_precision_scores = []\n",
    "\n",
    "rougel_f1_scores = []\n",
    "rougel_recall_scores = []\n",
    "rougel_precision_scores = []\n",
    "\n",
    "for i, score in scores_pd.iterrows():\n",
    "    rouge1_f1_scores.append(score['rouge-1']['f'])\n",
    "    rouge1_recall_scores.append(score['rouge-1']['r'])\n",
    "    rouge1_precision_scores.append(score['rouge-1']['p'])\n",
    "    \n",
    "    rouge2_f1_scores.append(score['rouge-2']['f'])\n",
    "    rouge2_recall_scores.append(score['rouge-2']['r'])\n",
    "    rouge2_precision_scores.append(score['rouge-2']['p'])\n",
    "    \n",
    "    rougel_f1_scores.append(score['rouge-l']['f'])\n",
    "    rougel_recall_scores.append(score['rouge-l']['r'])\n",
    "    rougel_precision_scores.append(score['rouge-l']['p'])\n",
    "    \n",
    "rouge1_f1_average = sum(rouge1_f1_scores)/len(rouge1_f1_scores)\n",
    "rouge1_recall_average = sum(rouge1_recall_scores)/len(rouge1_recall_scores)\n",
    "rouge1_precision_average = sum(rouge1_precision_scores)/len(rouge1_precision_scores)\n",
    "\n",
    "rouge2_f1_average = sum(rouge2_f1_scores)/len(rouge2_f1_scores)\n",
    "rouge2_recall_average = sum(rouge2_recall_scores)/len(rouge2_recall_scores)\n",
    "rouge2_precision_average = sum(rouge2_precision_scores)/len(rouge2_precision_scores)\n",
    "\n",
    "rougel_f1_average = sum(rougel_f1_scores)/len(rougel_f1_scores)\n",
    "rougel_recall_average = sum(rougel_recall_scores)/len(rougel_recall_scores)\n",
    "rougel_precision_average = sum(rougel_precision_scores)/len(rougel_precision_scores)\n",
    "\n",
    "\n",
    "res = {'r1': {'r': rouge1_recall_average, 'p': rouge1_precision_average, 'f1': rouge1_f1_average},\n",
    "      'r2': {'r': rouge2_recall_average, 'p': rouge2_precision_average, 'f1': rouge2_f1_average},\n",
    "      'rl': {'r': rougel_recall_average, 'p': rougel_precision_average, 'f1': rougel_f1_average}}\n",
    "\n",
    "pd.DataFrame(res).to_csv('TextRank_res.csv')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
