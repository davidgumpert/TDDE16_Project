{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Importing packages"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Importing all needed packages for the full notebook. Only needs to be ran once. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: ipynb in c:\\users\\david\\anaconda3\\envs\\tdde16\\lib\\site-packages (0.5.1)\n",
      "Requirement already satisfied: rouge-score in c:\\users\\david\\anaconda3\\envs\\tdde16\\lib\\site-packages (0.0.4)\n",
      "Requirement already satisfied: six>=1.14.0 in c:\\users\\david\\anaconda3\\envs\\tdde16\\lib\\site-packages (from rouge-score) (1.15.0)\n",
      "Requirement already satisfied: absl-py in c:\\users\\david\\anaconda3\\envs\\tdde16\\lib\\site-packages (from rouge-score) (0.13.0)\n",
      "Requirement already satisfied: numpy in c:\\users\\david\\anaconda3\\envs\\tdde16\\lib\\site-packages (from rouge-score) (1.19.2)\n",
      "Requirement already satisfied: nltk in c:\\users\\david\\anaconda3\\envs\\tdde16\\lib\\site-packages (from rouge-score) (3.5)\n",
      "Requirement already satisfied: joblib in c:\\users\\david\\anaconda3\\envs\\tdde16\\lib\\site-packages (from nltk->rouge-score) (1.0.0)\n",
      "Requirement already satisfied: tqdm in c:\\users\\david\\anaconda3\\envs\\tdde16\\lib\\site-packages (from nltk->rouge-score) (4.55.1)\n",
      "Requirement already satisfied: click in c:\\users\\david\\anaconda3\\envs\\tdde16\\lib\\site-packages (from nltk->rouge-score) (7.1.2)\n",
      "Requirement already satisfied: regex in c:\\users\\david\\anaconda3\\envs\\tdde16\\lib\\site-packages (from nltk->rouge-score) (2020.11.13)\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "!{sys.executable} -m pip install ipynb\n",
    "!{sys.executable} -m pip install rouge-score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gzip as gz\n",
    "import json\n",
    "import sys as sklearn\n",
    "import spacy as sp\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import math\n",
    "import re\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from rouge import Rouge\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Importing the data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Loading data from disc. Time consuming process that is only needed to be done once per session. After completion all needed data is saved in the *Data* variable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setting local file path\n",
    "path = \"newsroom-release/release/dev.jsonl.gz\"\n",
    "\n",
    "# Creating list entity to hold full set of loaded data\n",
    "data = []\n",
    "\n",
    "# Using gz to set path to zip file and iteritavly load each json line\n",
    "with gz.open(path) as f:\n",
    "    for ln in f:\n",
    "        obj = json.loads(ln)\n",
    "        data.append(obj)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Thinning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ïnitial data thinning to extract data entities suitable for an extractive summarization task. These are identified by looking at the entity property \"density_bin\" stating if summary is mainly \"extractive\" or \"abstractive\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>url</th>\n",
       "      <th>archive</th>\n",
       "      <th>title</th>\n",
       "      <th>date</th>\n",
       "      <th>text</th>\n",
       "      <th>summary</th>\n",
       "      <th>compression</th>\n",
       "      <th>coverage</th>\n",
       "      <th>density</th>\n",
       "      <th>compression_bin</th>\n",
       "      <th>coverage_bin</th>\n",
       "      <th>density_bin</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>http://www.nytimes.com/2006/06/05/technology/0...</td>\n",
       "      <td>http://web.archive.org/web/20060620021852id_/h...</td>\n",
       "      <td>India Becoming a Crucial Cog in the Machine at...</td>\n",
       "      <td>20060620021852</td>\n",
       "      <td>BANGALORE, India, June 4  The world's biggest...</td>\n",
       "      <td>India provides I.B.M. with its fastest-growing...</td>\n",
       "      <td>56.045455</td>\n",
       "      <td>0.954545</td>\n",
       "      <td>16.500000</td>\n",
       "      <td>high</td>\n",
       "      <td>high</td>\n",
       "      <td>extractive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>http://www.nydailynews.com/archives/news/1995/...</td>\n",
       "      <td>http://web.archive.org/web/20110210093603id_/h...</td>\n",
       "      <td>NEW YORKERS' ONLY REGRET WAS STAYING HOME</td>\n",
       "      <td>20110210093603</td>\n",
       "      <td>This story was reported by: NICK CHARLES, AUST...</td>\n",
       "      <td>As many black men marched on Washington yester...</td>\n",
       "      <td>6.152941</td>\n",
       "      <td>0.976471</td>\n",
       "      <td>24.600000</td>\n",
       "      <td>low</td>\n",
       "      <td>high</td>\n",
       "      <td>extractive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>http://www.reuters.com/article/2011/01/31/us-i...</td>\n",
       "      <td>http://web.archive.org/web/20120321005702id_/h...</td>\n",
       "      <td>Freed American hiker summoned back by Iran court</td>\n",
       "      <td>20120321005702</td>\n",
       "      <td>TEHRAN | Mon Jan 31, 2011 9:17am EST\\n\\nTEHRAN...</td>\n",
       "      <td>TEHRAN (Reuters) - An American woman who was f...</td>\n",
       "      <td>7.902439</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>39.048780</td>\n",
       "      <td>low</td>\n",
       "      <td>high</td>\n",
       "      <td>extractive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>http://www.reuters.com/article/2007/08/17/us-c...</td>\n",
       "      <td>http://web.archive.org/web/20120606165550id_/h...</td>\n",
       "      <td>Breast cancer vaccine looks safe, study shows</td>\n",
       "      <td>20120606165550</td>\n",
       "      <td>By Maggie Fox, Health and Science Editor\\n\\nWA...</td>\n",
       "      <td>WASHINGTON (Reuters) - A vaccine designed to t...</td>\n",
       "      <td>17.162162</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>35.054054</td>\n",
       "      <td>medium</td>\n",
       "      <td>high</td>\n",
       "      <td>extractive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>http://www.bostonglobe.com/arts/music/2014/01/...</td>\n",
       "      <td>http://web.archive.org/web/20140131020936id_/h...</td>\n",
       "      <td>Music review: Jake Bugg at the House of Blues</td>\n",
       "      <td>20140131020936</td>\n",
       "      <td>As the lights went down at the nearly sold-out...</td>\n",
       "      <td>As the lights went down at the nearly sold-out...</td>\n",
       "      <td>2.153061</td>\n",
       "      <td>0.994898</td>\n",
       "      <td>55.658163</td>\n",
       "      <td>low</td>\n",
       "      <td>high</td>\n",
       "      <td>extractive</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                  url  \\\n",
       "1   http://www.nytimes.com/2006/06/05/technology/0...   \n",
       "2   http://www.nydailynews.com/archives/news/1995/...   \n",
       "4   http://www.reuters.com/article/2011/01/31/us-i...   \n",
       "5   http://www.reuters.com/article/2007/08/17/us-c...   \n",
       "10  http://www.bostonglobe.com/arts/music/2014/01/...   \n",
       "\n",
       "                                              archive  \\\n",
       "1   http://web.archive.org/web/20060620021852id_/h...   \n",
       "2   http://web.archive.org/web/20110210093603id_/h...   \n",
       "4   http://web.archive.org/web/20120321005702id_/h...   \n",
       "5   http://web.archive.org/web/20120606165550id_/h...   \n",
       "10  http://web.archive.org/web/20140131020936id_/h...   \n",
       "\n",
       "                                                title            date  \\\n",
       "1   India Becoming a Crucial Cog in the Machine at...  20060620021852   \n",
       "2           NEW YORKERS' ONLY REGRET WAS STAYING HOME  20110210093603   \n",
       "4    Freed American hiker summoned back by Iran court  20120321005702   \n",
       "5       Breast cancer vaccine looks safe, study shows  20120606165550   \n",
       "10      Music review: Jake Bugg at the House of Blues  20140131020936   \n",
       "\n",
       "                                                 text  \\\n",
       "1   BANGALORE, India, June 4  The world's biggest...   \n",
       "2   This story was reported by: NICK CHARLES, AUST...   \n",
       "4   TEHRAN | Mon Jan 31, 2011 9:17am EST\\n\\nTEHRAN...   \n",
       "5   By Maggie Fox, Health and Science Editor\\n\\nWA...   \n",
       "10  As the lights went down at the nearly sold-out...   \n",
       "\n",
       "                                              summary  compression  coverage  \\\n",
       "1   India provides I.B.M. with its fastest-growing...    56.045455  0.954545   \n",
       "2   As many black men marched on Washington yester...     6.152941  0.976471   \n",
       "4   TEHRAN (Reuters) - An American woman who was f...     7.902439  1.000000   \n",
       "5   WASHINGTON (Reuters) - A vaccine designed to t...    17.162162  1.000000   \n",
       "10  As the lights went down at the nearly sold-out...     2.153061  0.994898   \n",
       "\n",
       "      density compression_bin coverage_bin density_bin  \n",
       "1   16.500000            high         high  extractive  \n",
       "2   24.600000             low         high  extractive  \n",
       "4   39.048780             low         high  extractive  \n",
       "5   35.054054          medium         high  extractive  \n",
       "10  55.658163             low         high  extractive  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Extracting appropriate data and transforming to pandas dataframe\n",
    "df = pd.DataFrame(data)\n",
    "df_extractive = df[df.density_bin == 'extractive']\n",
    "\n",
    "# Filtering on rough estimate of lenght text\n",
    "article_lengths = [len(text.split()) for text in df_extractive.text]\n",
    "_ = plt.hist(article_lengths, bins = 100, range = (0, 2000))\n",
    "length_check = [len > 250 for len in article_lengths]\n",
    "df_extractive = df_extractive[length_check]\n",
    "\n",
    "# Inspecting head of dataframe for inspection\n",
    "df_extractive.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Full dataset:  108837 \n",
      "Extractive dataset:  28347 \n",
      "Percentage:  0.26\n"
     ]
    }
   ],
   "source": [
    "# Printing the length of the dataset, before and after thinning\n",
    "print(\"Full dataset: \", len(df), \"\\nExtractive dataset: \",len(df_extractive), \"\\nPercentage: \", round(len(df_extractive)/len(df), 2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Intermidiate Input Representation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading the \"en_core_web_sm\" model from the SpaCy library\n",
    "# Disabling unused features for a lighter and more efficient model\n",
    "# The model is used for text transformation in preprocessing \n",
    "nlp = sp.load(\"en_core_web_sm\", disable=[\"tagger\", \"parser\", \"ner\", \"textcat\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sparse TF-IDF matrix representation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PREPROCESSING FUNCTION\n",
    "def preprocess(text):\n",
    "    # Cleans input text\n",
    "    text = nlp(text)\n",
    "    tokens = []\n",
    "    for token in text:\n",
    "        # Remove stopwords and numerics, append the lemma form to tokens list\n",
    "        if token.is_stop == False and token.is_alpha == True:\n",
    "            tokens.append(token.lemma_)\n",
    "    \n",
    "    # Return all lemmatized tokens in the input text\n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TF-IDF MATRIX CREATION FUNCTION\n",
    "def create_tfidf_matrix(corpus, preprocessor = preprocess):\n",
    "    #Initializing ScikitLearn TF-IDF vectorizer and creating TF-IDF sparse matrix\n",
    "    vectorizer = TfidfVectorizer(tokenizer = preprocessor)\n",
    "    tfidf_matrix = vectorizer.fit_transform(corpus['text'])\n",
    "    \n",
    "    # Saving list of all corpus tokens\n",
    "    feature_names = vectorizer.get_feature_names()\n",
    "\n",
    "    # Inspecting dimension of sparse matrix\n",
    "    # Rows should equal no. of df_extractive data entities\n",
    "    # Number of columns equals number of unique corpus tokens\n",
    "    print(\"TF-IDF matrix dimension: \", tfidf_matrix.get_shape(), \"\\nAligning with no. df_extractive enteties? \", tfidf_matrix.get_shape()[0] == len(df_extractive))\n",
    "    \n",
    "    # Returning TF-IDF matrix\n",
    "    return tfidf_matrix, feature_names"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Token and TF-IDF score pairing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TOKEN TFIDF PAIRING FUNCTION\n",
    "def token_tfidf_ranking(feature_names, tfidf_matrix, row_index):\n",
    "    # Exctracting indices of document tokens from the TF-IDF matrix\n",
    "    token_indices = tfidf_matrix[row_index,:].nonzero()[1]\n",
    "   \n",
    "    # Extract token names and pair with corresponding TF-IDF value from the TF-IDF matrix\n",
    "    # Sort by TF-IDF score\n",
    "    token_tfidf = pd.DataFrame(np.column_stack(([feature_names[index] for index in token_indices], [tfidf_matrix[row_index, x] for x in token_indices])), columns=['token', 'tfidf_score'])\n",
    "    token_tfidf = token_tfidf.sort_values(by='tfidf_score', ascending=False)\n",
    "    \n",
    "    #token_tfidf = token_tfidf.astype({\"word\": str, \"tfidf_score\": float})\n",
    "    \n",
    "    # Return the sorted list of (token, TF-IDF value) data frame\n",
    "    return token_tfidf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sentence splitting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading new model from SpaCy and adding sentencizer pipeline\n",
    "nlp_sentencizer = sp.load(\"en_core_web_sm\", disable=[\"tagger\", \"parser\", \"ner\", \"textcat\"])\n",
    "nlp_sentencizer.add_pipe(nlp.create_pipe('sentencizer'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SENTENCE SPLITTING FUNCTION\n",
    "def sentence_splitting(doc):\n",
    "    doc = nlp_sentencizer(doc['text'])\n",
    "    sentences = [sent.string.strip() for sent in doc.sents]\n",
    "    \n",
    "    return sentences"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sentence level tokenization and scoring"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SENTENCE SCORING FUNTION\n",
    "def sentence_scoring(sentences, token_tfidf_pairs):\n",
    "    sentence_scores = []\n",
    "    \n",
    "    for i, sentence in enumerate(sentences):\n",
    "        score = 0\n",
    "        sentence_length = len(sentence)\n",
    "\n",
    "        # Using preprocessing function to extract sentence tokens\n",
    "        sentence_tokens = preprocess(sentence)\n",
    "\n",
    "        # Summation of sentence tokens' TF-IDF values \n",
    "        for token in sentence_tokens:\n",
    "            token = token.lower()\n",
    "            if token in token_tfidf_pairs['token'].values:\n",
    "                score = score + float(token_tfidf_pairs.loc[token_tfidf_pairs['token'] == token]['tfidf_score'].values)\n",
    "\n",
    "        # Normalizing sentence score dependent on sentence length\n",
    "        score = score / sentence_length\n",
    "        \n",
    "        # Append to list of sentence scores\n",
    "        sentence_scores.append((i,score))\n",
    "    \n",
    "    # Save scores in pd dataframe\n",
    "    sentence_scores = pd.DataFrame(sentence_scores,columns=[\"sentence_index\", \"sentence_score\"]).sort_values(by='sentence_score', ascending=False)\n",
    "    \n",
    "    # Return final sentence scores\n",
    "    return sentence_scores "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sentence Extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SENTENCE EXTRACTION FUNCTION\n",
    "def sentence_extraction(sentences, sentence_scores, n):\n",
    "    summary = []\n",
    "    \n",
    "    # Extracting indices of the n number of top scoring sentences\n",
    "    # sort them in ascending order\n",
    "    top_sentence_indices = np.sort(sentence_scores[0:n]['sentence_index'].values)\n",
    "    \n",
    "    # Extracting the original sentences and appending to summary list\n",
    "    for index in top_sentence_indices:\n",
    "        summary.append(sentences[index])\n",
    "        \n",
    "    # Joining summary sentences\n",
    "    summary = ' '.join(summary)\n",
    "    \n",
    "    return summary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Score summaries with ROUGE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ROUGE SCORING FUNCTION\n",
    "def rouge_blue_scoring(summaries, references):\n",
    "    rouge_score = rouge.get_scores(summaries, references)\n",
    "    return rouge_score\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Running the Experiment\n",
    "* Preprocess and Vectorize\n",
    "* Extract document's tokens and tfidf key-value pairs\n",
    "* Sentencize document and score each seperate sentence\n",
    "* Extract top *n* sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Intermidiate Input Representation\n",
    "corpus = df_extractive\n",
    "corpus.reset_index(inplace = True)\n",
    "tfidf_matrix, feature_names = create_tfidf_matrix(corpus = corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sentence Scoring and Sentence Extraction\n",
    "summaries = []\n",
    "\n",
    "for index, doc in corpus.iterrows():\n",
    "    token_tfidf_pairs = token_tfidf_ranking(feature_names, tfidf_matrix, index)\n",
    "    sentences = sentence_splitting(doc)\n",
    "    sentence_scores = sentence_scoring(sentences, token_tfidf_pairs)\n",
    "    summary = sentence_extraction(sentences, sentence_scores, 3)\n",
    "    summaries.append(summary)\n",
    "    if(index != 0 and index % 1000 == 0):\n",
    "        print('Checkpoint: ', index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ROUGE Scoring \n",
    "rouge = Rouge()\n",
    "scores = rouge_blue_scoring(summaries, corpus.iloc[0:len(summaries)].summary.values.tolist())\n",
    "scores_pd = pd.DataFrame(scores)\n",
    "\n",
    "rouge1_f1_scores = []\n",
    "rouge1_recall_scores = []\n",
    "rouge1_precision_scores = []\n",
    "\n",
    "rouge2_f1_scores = []\n",
    "rouge2_recall_scores = []\n",
    "rouge2_precision_scores = []\n",
    "\n",
    "rougel_f1_scores = []\n",
    "rougel_recall_scores = []\n",
    "rougel_precision_scores = []\n",
    "\n",
    "for i, score in scores_pd.iterrows():\n",
    "    rouge1_f1_scores.append(score['rouge-1']['f'])\n",
    "    rouge1_recall_scores.append(score['rouge-1']['r'])\n",
    "    rouge1_precision_scores.append(score['rouge-1']['p'])\n",
    "    \n",
    "    rouge2_f1_scores.append(score['rouge-2']['f'])\n",
    "    rouge2_recall_scores.append(score['rouge-2']['r'])\n",
    "    rouge2_precision_scores.append(score['rouge-2']['p'])\n",
    "    \n",
    "    rougel_f1_scores.append(score['rouge-l']['f'])\n",
    "    rougel_recall_scores.append(score['rouge-l']['r'])\n",
    "    rougel_precision_scores.append(score['rouge-l']['p'])\n",
    "    \n",
    "rouge1_f1_average = sum(rouge1_f1_scores)/len(rouge1_f1_scores)\n",
    "rouge1_recall_average = sum(rouge1_recall_scores)/len(rouge1_recall_scores)\n",
    "rouge1_precision_average = sum(rouge1_precision_scores)/len(rouge1_precision_scores)\n",
    "\n",
    "rouge2_f1_average = sum(rouge2_f1_scores)/len(rouge2_f1_scores)\n",
    "rouge2_recall_average = sum(rouge2_recall_scores)/len(rouge2_recall_scores)\n",
    "rouge2_precision_average = sum(rouge2_precision_scores)/len(rouge2_precision_scores)\n",
    "\n",
    "rougel_f1_average = sum(rougel_f1_scores)/len(rougel_f1_scores)\n",
    "rougel_recall_average = sum(rougel_recall_scores)/len(rougel_recall_scores)\n",
    "rougel_precision_average = sum(rougel_precision_scores)/len(rougel_precision_scores)\n",
    "\n",
    "\n",
    "res = {'r1': {'r': rouge1_recall_average, 'p': rouge1_precision_average, 'f1': rouge1_f1_average},\n",
    "      'r2': {'r': rouge2_recall_average, 'p': rouge2_precision_average, 'f1': rouge2_f1_average},\n",
    "      'rl': {'r': rougel_recall_average, 'p': rougel_precision_average, 'f1': rougel_f1_average}}\n",
    "\n",
    "pd.DataFrame(res).to_csv('TFIDF_res.csv')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
